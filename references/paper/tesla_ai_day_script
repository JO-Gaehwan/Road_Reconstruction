이 소스들을 입력한 이유는 다름아닌 Vehicle Driving Road Reconstruction for Lane Auto Labeling Project
를 진행하기 위해서이다. 테슬라의 방식을 이해하고 활용하기 위함이다.
아래의 스크립트에 주목하고 있다.
----------------
Now, in the beginning, roughly three or four years ago, most of our labeling was in image space. And so, you can imagine that this is taking quite some time to annotate an image like this. And this is what it looked like, where we are sort of drawing polygons and polylines on top of these single individual images. As I mentioned, we need millions of vector space labels, so this is not going to cut it. So, very quickly we graduated to three-dimensional or four-dimensional labeling, where we are directly labeling in vector space, not in individual images.
Link
(1:27:04) So here, what I’m showing is a clip, and you are seeing a very small reconstruction – you’re about to see a lot more reconstructions soon – but it’s very small reconstruction of the ground plane on which the car drove, and a little bit of the point cloud here that was reconstructed. And what you’re seeing here is that the labeler is changing the labels directly in vector space. And then we are reprojecting those changes into camera images. So, we’re labeling directly in vector space. And this gave us a massive increase in throughput for a lot of our labels, because you label once in 3D, and then you get to reproject.
But even this, we realized, was actually not going to cut it because people and computers have different pros and cons. People are extremely good at things like semantics, but computers are very good at geometry, reconstruction, triangulation, tracking. And so really, for us, it’s much more becoming a story of how do humans and computers collaborate to actually create these vector space data sets. And so, we’re going to now talk about auto labeling, which is some of the infrastructure we’ve developed for labeling these clips at scale.
Ashok Elluswamy: (1:28:09) Hi again. So even though we have lots of human labelers, the amount of training that are needed for training the networks significantly outnumbers them. So we tried to invest in a massive auto labeling pipeline. Here’s an example of how we label a single clip.
A clip is an entity that has dense sensor data, like videos, IMU radar, GPS, odometry, etc. This can be 45 seconds to a minute long. These can be uploaded from our own engineering cars or from customer cars. We collect these clips, and then send them to our servers, where we run a lot of neural networks offline to produce intermediate results like segmentation masks, depth, point matching etc. This then goes through a lot of robotics and AI algorithm to produce a final set of labels that can be used to train the networks.
(1:28:55) First, we want to label the road surface. Typically, we can use splines or meshes to represent the road surface, but those are – because of the topology restrictions – are not differentiable and not amenable to producing this. What we do instead is in the style of neural radiance fields work from last year, which is quite popular… – we use an implicit representation to represent the road surface. Here, we are querying xy points on the ground and asking the network to predict the height of the ground surface along with various semantics such as curbs, lane boundaries, road surface, driving space, etc.
So, given a single xy we get a z. Together they make a 3D point, and they can be reprojected into all the camera views. We make millions of such queries and get lots of points. These points are reprojected into all the camera views. We are showing on the top right here, one such camera image with all these points we projected. Now we can compare this reprojected point with the image-based prediction of the segmentations, and jointly optimizing this for all the camera views both across space and time, produced an excellent reconstruction.
Link
Here’s an example of how that looks like. So here, this is an optimized road surface, that reproduction to the eight cameras that the car has, and across all of time, and you can see how it’s consistent across both space and time.
Link
(1:30:17) A single car driving through some location can sweep out some patch around the trajectory using this technique. But we don’t have to stop there. So here, we collected different clips from the same location from different cars, maybe, and each of them sweeps out some part of the road. The cool thing is, we can bring them all together into a single giant optimization. So here, the 16 different trips are organized, all aligned using various features such as roadages, lane lines – all of them should agree with each other, and also agree with all of the image-based observations.